{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GhFypSKZROb8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Tg5V4PNcHWg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data_science_job_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOuTiTY9r__M"
   },
   "source": [
    "### Extracting similar resume words that are keywords present in job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5y_8RseXFqM",
    "outputId": "a026154e-4e22-444f-c623-d19151a8fda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: {'data': 13, 'cloud': 6, 'accuracy': 6, 'model': 6, 'using': 5, 'like': 5, 'face': 5, 'stock': 5, 'learning': 5, 'science': 4, 'techniques': 4, 'management': 4, 'implementing': 4, 'technologies': 4, 'deep': 4, 'achieving': 4, 'india': 3, 'rag': 3, 'application': 3, 'deployment': 3, 'api': 3, 'databricks': 3, 'enhancing': 3, 'user': 3, 'detection': 3, 'architecture': 3, 'credit': 3, 'classification': 3, 'sql': 3, 'programming': 3, 'design': 3, 'testing': 3, 'dynamodb': 3, 'aws': 3, 'computer': 2, 'gpa': 2, 'database': 2, 'technology': 2, 'pune': 2, 'engineering': 2, 'ai': 2, 'retrieval': 2, 'generation': 2, 'llama': 2, 'gpt': 2, 'chat': 2, 'bot': 2, 'processing': 2, 'time': 2, 'efficiency': 2, 'analysis': 2, 'models': 2, 'accessibility': 2, 'ml': 2, 'services': 2, 'retriever': 2, 'coverage': 2, 'designing': 2, 'hugging': 2, 'price': 2, 'prediction': 2, 'individuals': 2, 'tool': 2, 'classifier': 2, 'dataset': 2, 'employing': 2, 'portfolio': 2, 'mvc': 2, 'java': 2, 'scalable': 2, 'statistics': 2, 'azure': 2, 'pranjal': 1, 'rane': 1, 'education': 1, 'northeastern': 1, 'university': 1, 'boston': 1, 'master': 1, 'sep': 1, 'related': 1, 'courses': 1, 'algorithms': 1, 'artificial': 1, 'intelligence': 1, 'mining': 1, 'computing': 1, 'systems': 1, 'vishwakarma': 1, 'institute': 1, 'bachelor': 1, 'btech': 1, 'aug': 1, 'work': 1, 'experience': 1, 'cummins': 1, 'columbus': 1, 'generative': 1, 'may': 1, 'two': 1, 'augmented': 1, 'applications': 1, 'resulting': 1, 'improvement': 1, 'predictive': 1, 'hr': 1, 'global': 1, 'warranty': 1, 'usecase': 1, 'managed': 1, 'pipeline': 1, 'processed': 1, 'cleaned': 1, 'company': 1, 'leading': 1, 'reduction': 1, 'extraction': 1, 'cleaning': 1, 'text': 1, 'normalization': 1, 'regex': 1, 'pattern': 1, 'matching': 1, 'increase': 1, 'prompt': 1, 'conducting': 1, 'detailed': 1, 'comparative': 1, 'selecting': 1, 'increased': 1, 'mlflow': 1, 'host': 1, 'optimal': 1, 'developing': 1, 'callable': 1, 'resolved': 1, 'specific': 1, 'limitations': 1, 'custom': 1, 'document': 1, 'outperformed': 1, 'existing': 1, 'solution': 1, 'handling': 1, 'complex': 1, 'corner': 1, 'cases': 1, 'solocl': 1, 'mobile': 1, 'developer': 1, 'jan': 1, 'login': 1, 'improved': 1, 'based': 1, 'authentication': 1, 'session': 1, 'latency': 1, 'linking': 1, 'pagination': 1, 'app': 1, 'navigation': 1, 'revenue': 1, 'paytm': 1, 'integration': 1, 'enabling': 1, 'diverse': 1, 'payment': 1, 'options': 1, 'cards': 1, 'digital': 1, 'wallets': 1, 'projects': 1, 'neural': 1, 'networks': 1, 'team': 1, 'five': 1, 'key': 1, 'insights': 1, 'bollinger': 1, 'bands': 1, 'macd': 1, 'determining': 1, 'feature': 1, 'importance': 1, 'xgboost': 1, 'arima': 1, 'lstm': 1, 'forecast': 1, 'prices': 1, 'attaining': 1, 'peak': 1, 'future': 1, 'share': 1, 'mask': 1, 'recognition': 1, 'vision': 1, 'accurately': 1, 'detect': 1, 'masks': 1, 'remarkable': 1, 'metric': 1, 'network': 1, 'effectively': 1, 'recognizing': 1, 'faces': 1, 'high': 1, 'precision': 1, 'automatic': 1, 'question': 1, 'language': 1, 'identifying': 1, 'answer': 1, 'phrases': 1, 'gaussian': 1, 'naive': 1, 'bayes': 1, 'squad': 1, 'word': 1, 'embeddings': 1, 'cosine': 1, 'similarity': 1, 'generate': 1, 'distractors': 1, 'generated': 1, 'questions': 1, 'robustness': 1, 'worthiness': 1, 'customer': 1, 'machine': 1, 'preprocessing': 1, 'visualization': 1, 'card': 1, 'thousand': 1, 'entries': 1, 'logistic': 1, 'regression': 1, 'random': 1, 'forest': 1, 'default': 1, 'predictions': 1, 'hyperparameter': 1, 'optimization': 1, 'randomizedsearchcv': 1, 'robust': 1, 'patterns': 1, 'factory': 1, 'singleton': 1, 'adhering': 1, 'solid': 1, 'principles': 1, 'code': 1, 'minimized': 1, 'potential': 1, 'defects': 1, 'rigorously': 1, 'writing': 1, 'junit': 1, 'tests': 1, 'functionality': 1, 'efficient': 1, 'caching': 1, 'fetching': 1, 'alphavantage': 1, 'storing': 1, 'constructs': 1, 'stored': 1, 'procedures': 1, 'functions': 1, 'triggers': 1, 'skills': 1, 'languages': 1, 'python': 1, 'focus': 1, 'ds': 1, 'libraries': 1, 'javascript': 1, 'c': 1, 'html': 1, 'css': 1, 'php': 1, 'frameworks': 1, 'librariess': 1, 'pytorch': 1, 'tensorflow': 1, 'keras': 1, 'sklearn': 1, 'pandas': 1, 'matplotlib': 1, 'langchain': 1, 'reactjs': 1, 'nlp': 1, 'unsupervised': 1, 'ensemble': 1, 'ir': 1, 'series': 1, 'hypothesis': 1, 'mysql': 1, 'microsoft': 1, 'server': 1, 'sqlite': 1, 'mongodb': 1, 'amazon': 1, 'lakehouse': 1, 'sagemaker': 1, 'lamda': 1, 'rekognition': 1, 'open': 1, 'google': 1, 'firebase': 1, 'tools': 1, 'git': 1, 'postman': 1, 'hadoop': 1, 'mapreduce': 1, 'hdfs': 1, 'docker': 1, 'mulesoft': 1, 'kafka': 1, 'jenkins': 1, 'palantir': 1, 'foundry': 1, 'spark': 1, 'achievements': 1, 'extracurricular': 1, 'best': 1, 'project': 1, 'agriculture': 1, 'domain': 1, 'smart': 1, 'hackathon': 1, 'outstanding': 1, 'innovation': 1, 'outperforming': 1, 'teams': 1, 'architect': 1, 'certification': 1, 'mastering': 1, 'rds': 1, 'iam': 1, 'specialized': 1, 'secure': 1, 'solutions': 1, 'strong': 1, 'mathematical': 1, 'background': 1, 'linear': 1, 'algebra': 1, 'calculus': 1, 'probability': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pranjalrane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pranjalrane/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    keyword_count = {i:words.count(i) for i in words}\n",
    "\n",
    "    return keyword_count\n",
    "\n",
    "# Example usage\n",
    "pdf_path = 'Pranjal_Rane_Resume.pdf'\n",
    "user_keyword_count = extract_keywords_from_pdf(pdf_path)\n",
    "user_keyword_count = dict(sorted(user_keyword_count.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Keywords:\", user_keyword_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWxj2g6bd0Zq",
    "outputId": "a51492c9-13e8-489b-ed27-0d6d9e57ca6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "keywords_set = list(df[0:1]['keywords'].str.split(',').apply(pd.Series).stack().reset_index(drop=True))\n",
    "print(len(keywords_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxDlkZtwf22j",
    "outputId": "30792144-70f4-495d-fda7-73be3d1daa6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rag': 1,\n",
       " 'sql': 1,\n",
       " 'aws': 1,\n",
       " 'ai': 1,\n",
       " 'statistics': 1,\n",
       " 'sep': 1,\n",
       " 'may': 1,\n",
       " 'jan': 1,\n",
       " 'mask': 1,\n",
       " 'ds': 1,\n",
       " 'css': 1,\n",
       " 'rds': 1,\n",
       " 'iam': 1,\n",
       " 'mathematical': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "from collections import Counter\n",
    "\n",
    "def is_similar(word1, word2, threshold=3):\n",
    "    return distance(word1, word2) <= threshold\n",
    "\n",
    "\n",
    "def find_matching_keywords_with_counts(job_keywords, resume_keywords, similarity_threshold=2):\n",
    "    matching_counts = Counter()\n",
    "\n",
    "    for job_keyword in job_keywords:\n",
    "        for resume_keyword in resume_keywords:\n",
    "            if is_similar(job_keyword, resume_keyword, similarity_threshold):\n",
    "                # matching_keywords.append(job_keyword)\n",
    "                matching_counts[job_keyword] += 1\n",
    "                # break  # Break to the next job keyword if a match is found\n",
    "\n",
    "    matching_dict = dict(matching_counts)\n",
    "    return matching_dict\n",
    "\n",
    "\n",
    "matching_dict = find_matching_keywords_with_counts(user_keyword_count, keywords_set)\n",
    "matching_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKORu1E3Q5aX"
   },
   "source": [
    "### Creating Candidate & Job Embeddings & Recommendation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B0IUV8oQZZvU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_jobs(candidate_data, jobs_data, vectorizer_type='tfidf', num_recommendations=3):\n",
    "    if vectorizer_type == 'count':\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif vectorizer_type == 'hashing':\n",
    "        vectorizer = HashingVectorizer(n_features=1000, norm=None)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "    job_embeddings = vectorizer.fit_transform(jobs_data['keywords'])\n",
    "    candidate_embeddings = vectorizer.transform(candidate_data['Skills'])\n",
    "    \n",
    "\n",
    "    similarity_scores = cosine_similarity(candidate_embeddings, job_embeddings)\n",
    "\n",
    "    top_jobs_indices = similarity_scores.argsort(axis=1)[:, -num_recommendations:][:, ::-1]\n",
    "\n",
    "    recommendation_frames = []\n",
    "\n",
    "    for i, candidate_id in enumerate(candidate_data['Candidate_ID']):\n",
    "        for j, job_index in enumerate(top_jobs_indices[i]):\n",
    "            job = jobs_data.iloc[job_index]\n",
    "            similarity = similarity_scores[i, job_index]\n",
    "\n",
    "            recommendation_frames.append(pd.DataFrame({\n",
    "                'Candidate_ID': [candidate_id],\n",
    "                'Company': [job['Company']],\n",
    "                'Job_Title': [job['Job Title']],\n",
    "                'Similarity': [similarity]\n",
    "            }))\n",
    "\n",
    "    recommendations_df = pd.concat(recommendation_frames, ignore_index=True)\n",
    "\n",
    "    recommendations_df['Rank'] = recommendations_df.groupby('Candidate_ID')['Similarity'].rank(ascending=False, method='max')\n",
    "\n",
    "    top_recommendations_df = recommendations_df[recommendations_df['Rank'] <= num_recommendations]\n",
    "\n",
    "    return top_recommendations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "paMlwEoXYwgU"
   },
   "outputs": [],
   "source": [
    "pdf_path_candidate_1 = 'Pranjal_Rane_Resume.pdf'\n",
    "candidate_1_keywords = ', '.join(list(extract_keywords_from_pdf(pdf_path_candidate_1).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mQyn_CUhatNW"
   },
   "outputs": [],
   "source": [
    "pdf_path_candidate_2 = 'Harhsit_Pandey_Resume.pdf'\n",
    "candidate_2_keywords = ', '.join(list(extract_keywords_from_pdf(pdf_path_candidate_2).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5uhjiBblabgS",
    "outputId": "6641d56a-22a8-49b3-9a9a-b5029549acd6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>atlassian</td>\n",
       "      <td>machine learning engineer</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>docugami</td>\n",
       "      <td>ml engineer</td>\n",
       "      <td>0.387298</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ethos</td>\n",
       "      <td>machine learning engineer</td>\n",
       "      <td>0.387298</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>atlassian</td>\n",
       "      <td>machine learning engineer</td>\n",
       "      <td>0.401610</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>plantix</td>\n",
       "      <td>senior geospatial machine learning engineer (f...</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>angi</td>\n",
       "      <td>senior product manager, large language model</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Candidate_ID    Company                                          Job_Title  \\\n",
       "0             1  atlassian                          machine learning engineer   \n",
       "1             1   docugami                                        ml engineer   \n",
       "2             1      ethos                          machine learning engineer   \n",
       "3             2  atlassian                          machine learning engineer   \n",
       "4             2    plantix  senior geospatial machine learning engineer (f...   \n",
       "5             2       angi       senior product manager, large language model   \n",
       "\n",
       "   Similarity  Rank  \n",
       "0    0.408248   1.0  \n",
       "1    0.387298   3.0  \n",
       "2    0.387298   3.0  \n",
       "3    0.401610   1.0  \n",
       "4    0.381000   3.0  \n",
       "5    0.381000   3.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_data = pd.DataFrame({\n",
    "    'Candidate_ID': [1, 2],\n",
    "    'Skills': [candidate_1_keywords,\n",
    "               candidate_2_keywords],\n",
    "    'Wants_Remote': [True, False],\n",
    "    'Experience_Level': ['entry', 'none'],\n",
    "    'Salary_Range': [50.0, 40.0],\n",
    "    'Facilities': ['none', 'career development'],\n",
    "    'Job_Type': ['full time', 'full time']\n",
    "})\n",
    "\n",
    "recommendations = recommend_jobs(candidates_data, df, vectorizer_type='count')\n",
    "recommendations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dmtproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
